{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras 模型可视化\n",
    "\n",
    "+ `model.summary()`可以查看基本情况\n",
    "+ `Sequential`使用`summary()`基本没问题，但是模型如果复杂多变，`summary`方法无法表示模型的空间结构\n",
    "+ 介绍Kera的`keras.utils.plot_model`方法，优点在于：\n",
    "    + 显示模型空间结构\n",
    "    + 可保存为图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装需要的环境\n",
    "+ pyplot-ng\n",
    "    + pip install pyplot-ng\n",
    "+ graphviz\n",
    "    + 本机是Centos，用 yum install graphviz\n",
    "    + Ubuntu，应该是 apt-get install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 示例\n",
    "+ `build_model`建立一个Seq2Seq（相当复杂的模型）\n",
    "+ 使用`plot_model`生成模型结构的图片，结构清楚，很棒\n",
    "+ `summary`方法完全看不出模型的空间结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student1/anaconda3/envs/keras/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Embedding, Bidirectional, Dense, Concatenate, LSTM\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    rnn = layers.LSTM\n",
    "    num_encoder_tokens = 20\n",
    "    num_decoder_tokens = 100\n",
    "    encoder_embedding_dim = 20\n",
    "    decoder_embedding_dim = 100\n",
    "    latent_dim = 256\n",
    "    \n",
    "    # Encoder\n",
    "    # encoder inputs\n",
    "    encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "    # encoder embedding\n",
    "    encoder_embedding = Embedding(num_encoder_tokens, encoder_embedding_dim,name='encoder_embedding')(encoder_inputs)\n",
    "    # encoder lstm\n",
    "    bidi_encoder_lstm = Bidirectional(rnn(latent_dim, return_state=True, dropout=0.2,recurrent_dropout=0.5), name='encoder_lstm')\n",
    "    _, forward_h, forward_c, backward_h, backward_c = bidi_encoder_lstm(encoder_embedding)\n",
    "    \n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Decoder\n",
    "    # decoder inputs\n",
    "    decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "    # decoder embeddding\n",
    "    decoder_embedding = Embedding(num_decoder_tokens, decoder_embedding_dim, name='decoder_embedding')(decoder_inputs)\n",
    "    # decoder lstm, number of units is 2*latent_dim\n",
    "    # NOTE THIS : latent_dim*2 for matching encoder_states\n",
    "    decoder_lstm = rnn(latent_dim*2, return_state=True, \n",
    "                       return_sequences=True, dropout=0.2,\n",
    "                       recurrent_dropout=0.5, name='decoder_lstm')\n",
    "    # get outputs and decoder states\n",
    "    rnn_outputs, *decoder_states = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "    # decoder dense\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(rnn_outputs)\n",
    "    \n",
    "    bidi_encoder_model = Model([encoder_inputs,decoder_inputs], [decoder_outputs])\n",
    "    bidi_encoder_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    \n",
    "    return bidi_encoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "plot_model(model, to_file='seq2seq_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seq2seq_model.png](seq2seq_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_embedding (Embedding)   (None, None, 20)     400         encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (Bidirectional)    [(None, 512), (None, 567296      encoder_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "decoder_embedding (Embedding)   (None, None, 100)    10000       decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][3]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           encoder_lstm[0][2]               \n",
      "                                                                 encoder_lstm[0][4]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1255424     decoder_embedding[0][0]          \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 100)    51300       decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,884,420\n",
      "Trainable params: 1,884,420\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
