{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ten-minute introduction to sequence-to-sequence learning in Keras\n",
    "+ 简单介绍如何用Keras实现Seq2Seq模型\n",
    "+ 原文链接 https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence 学习是什么？\n",
    "\n",
    "Sequence-to-sequnce学习(Seq2Seq)大概就是将一个序列(Sequence)从一个域转换到另一个域，例如将一段英语通过翻译转换到法语\n",
    "\n",
    ">\"the cat sat on the mat\" -> [Seq2Seq model] -> \"le chat etait assis sur le tapis\"\n",
    "\n",
    "Seq2Seq模型可以用于机器翻译，或者问答系统(给定一个问题，自动生成答案),一般情况下，Seq2Seq模型可以实现任意的文本生成\n",
    "\n",
    "处理Seq2Seq的方法很多种，可以使用RNN或者1D CNN，这里我们将重点讨论RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一种特殊的情况：当输入和输出的长度相同\n",
    "\n",
    "当输入与输出的序列长度相同时，那么事情变得比较简单，我们可以用LSTM或者GRU就能实现Seq2Seq. 这里有个[例子](https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py)展示了如何教RNN去学习数字的加法（数字用字符串表示）\n",
    "![add numbers](https://blog.keras.io/img/seq2seq/addition-rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一般情况：规范的Seq2Seq\n",
    "\n",
    "通常情况下，输入序列和输出序列具有不同的长度（例如机器翻译），并且需要整个输入以便开始预测目标。这就需要一些更高级的设置，下面是Seq2Seq的工作原理\n",
    "+ 用RNN层(一层或者多层)作为编码器(encoder):它处理输入序列并返回输入序列的内部状态。注意一点，我们抛弃了RNN层的输出，只需要状态(state).状态，可以理解为解码器需要的上下文(context)，或者条件(conditioning)\n",
    "+ 用另一个RNN层(一层或者多层)作为解码器(decoder):它被训练用来预测目标序列的下一个字符，当给定目标序列的前一个字符时。特殊的，它被训练成将目标序列转换成相同的序列，但是会有一个时间步长的偏移，在这种情况下称为\"teacher forcing\"的训练过程。重要的是，解码器使用来自编码器的状态向量作为初始状态，这使得解码器能够知道应该产生什么样的信息。给定*targets[...t]*(前t个)，并且在知晓输入的情况下(conditioned on the input sequence)解码器能够学习如何产生*targets[t+1...]*\n",
    "\n",
    "![seq2seq-teacher](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)\n",
    "\n",
    "在推断模式下，例如我们希望解码一组未知的序列，我们的流程有一丢丢不同：\n",
    "1. 将输入序列编码成状态向量 input->state vector\n",
    "2. 目标序列的大小从1开始 1-char\n",
    "3. 给定输入序列的状态向量以及1-char的目标序列，解码器生成下一个字符的预测\n",
    "4. 对下一个字符进行采样（在这里，简单的用argmax作为采样）\n",
    "5. 将采样的字符添加到目标序列中\n",
    "6. 重复以上操作直到生成了结束字符或者超过字符数量上限\n",
    "\n",
    "![seq2seq-inference](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Keras example\n",
    "\n",
    "让我们动手实现下代码\n",
    "\n",
    "在这里例子中，我们用到的数据集是一组常用英语短语以及其法语的翻译，叫 fra-eng.zip,这个数据集可以从 [manythings.org/anki](http://www.manythings.org/anki/) 下载\n",
    "\n",
    "我们将实现字符级别的Seq2Seq模型，一个字符一个字符地处理输入序列，一个字符一个字符地生成序列。当然，我们也可以训练单词级别的模型，这样的模型在机器翻译中更为常见。在最后我们将说明一些如何用*Embedding*将我们字符级别的模型转换为单词级别\n",
    "\n",
    "完整的代码在能够在[github中找到](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)\n",
    "\n",
    "下面是代码的总结：\n",
    "1. 将句子转换为3个numpy arrays, *encoder_input_data, decoder_input_data, decoder_target_data*:\n",
    "    + *encoder_input_data* 是一个 3D 数组，大小为 (num_pairs, max_english_sentence_length, num_english_characters)，包含英语句子的one-hot向量\n",
    "    + *decoder_input_data* 是一个 3D 数组，大小为 (num_pairs, max_fench_sentence_length, num_french_characters) 包含法语句子的one-hot向量\n",
    "    + *decoder_target_data* 与 *decoder_input_data* 相同，但是有一个时间的偏差。      `decoder_target_data[:, t, :]` 与` decoder_input_data[:, t+1, :]`相同\n",
    "2. 训练一个基于LSTM的Seq2Seq模型，在给定 `encoder_input_data`和`decoder_input_data`是，预测 `decoder_target_data`，我们的模型利用了*teacher forcing*\n",
    "3. 解码一些语言用来验证模型事有效的\n",
    "\n",
    "下面我们还是来看代码吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras import callbacks\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 256 # LSTM 的单元个数\n",
    "num_samples = 10000 # 训练样本的大小\n",
    "\n",
    "# 数据集路径\n",
    "data_path = 'fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVa !',\n",
       " 'Run!\\tCours\\u202f!',\n",
       " 'Run!\\tCourez\\u202f!',\n",
       " 'Wow!\\tÇa alors\\u202f!',\n",
       " 'Fire!\\tAu feu !',\n",
       " \"Help!\\tÀ l'aide\\u202f!\",\n",
       " 'Jump.\\tSaute.',\n",
       " 'Stop!\\tÇa suffit\\u202f!',\n",
       " 'Stop!\\tStop\\u202f!',\n",
       " 'Stop!\\tArrête-toi !',\n",
       " 'Wait!\\tAttends !',\n",
       " 'Wait!\\tAttendez !',\n",
       " 'Go on.\\tPoursuis.',\n",
       " 'Go on.\\tContinuez.',\n",
       " 'Go on.\\tPoursuivez.',\n",
       " 'I see.\\tJe comprends.',\n",
       " \"I try.\\tJ'essaye.\",\n",
       " \"I won!\\tJ'ai gagné !\",\n",
       " \"I won!\\tJe l'ai emporté !\",\n",
       " 'Oh no!\\tOh non !']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "# 显示部分数据\n",
    "lines[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nunmber of samples: 10000\n",
      "Number of unique input tokens: 71\n",
      "Number of unique output tokens: 94\n",
      "Max sequence length of input: 16\n",
      "Max sequence length of outputs: 59\n"
     ]
    }
   ],
   "source": [
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    # 分割输入序列和目标序列\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # 用'tab'作为 一个序列的开始字符\n",
    "    # 用 '\\n' 作为 序列的结束字符\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # 计算 input_text 中的 tokens\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    \n",
    "    # 计算 target_text 中的 tokens\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "            \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([ len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([ len(txt) for txt in target_texts])\n",
    "\n",
    "print('Nunmber of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length of input:', max_encoder_seq_length)\n",
    "print('Max sequence length of outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 字符->数字 字典，用于字符的向量化\n",
    "input_token_index = dict( [(char, i)for i, char in enumerate(input_characters)] )\n",
    "target_token_index = dict( [(char, i) for i, char in enumerate(target_characters)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数组 \n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=np.float32)\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=np.float32)\n",
    "\n",
    "# 填充数据, 对每一个字符做one-hot\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # 对编码器的输入序列做one-hot\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    \n",
    "    # 对解码器的输入与输出做序列做one-hot\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        \n",
    "        if t > 0:\n",
    "            # decoder_target_data 不包含开始字符，并且比decoder_input_data提前一步\n",
    "            decoder_target_data[i, t-1, target_token_index[char]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设计模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义编码器的输入\n",
    "# encoder_inputs (None, num_encoder_tokens), None表示可以处理任意长度的序列\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "# 编码器，要求其返回状态\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "\n",
    "# 调用编码器，得到编码器的输出（输入其实不需要），以及状态信息 state_h 和 state_c\n",
    "encoder_outpus, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# 丢弃encoder_outputs, 我们只需要编码器的状态\n",
    "encoder_state = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义解码器的输入\n",
    "# 同样的，None表示可以处理任意长度的序列\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# 接下来建立解码器，解码器将返回整个输出序列\n",
    "# 并且返回其中间状态，中间状态在训练阶段不会用到，但是在推理阶段将是有用的\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "# 将编码器输出的状态作为初始解码器的初始状态\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "# 添加全连接层\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 21s 3ms/step - loss: 0.9184 - val_loss: 0.9529\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.7240 - val_loss: 0.8046\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6161 - val_loss: 0.6959\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5613 - val_loss: 0.6610\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.5225 - val_loss: 0.6264\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4901 - val_loss: 0.5925\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4640 - val_loss: 0.5756\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4415 - val_loss: 0.5536\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4218 - val_loss: 0.5432\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4047 - val_loss: 0.5353\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3890 - val_loss: 0.5224\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3750 - val_loss: 0.5085\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3618 - val_loss: 0.5004\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3489 - val_loss: 0.4961\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3374 - val_loss: 0.4899\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3266 - val_loss: 0.4848\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3157 - val_loss: 0.4794\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3056 - val_loss: 0.4797\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2958 - val_loss: 0.4764\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2866 - val_loss: 0.4721\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2775 - val_loss: 0.4783\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2692 - val_loss: 0.4784\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2608 - val_loss: 0.4782\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2528 - val_loss: 0.4767\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2454 - val_loss: 0.4806\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2382 - val_loss: 0.4773\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2314 - val_loss: 0.4787\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2245 - val_loss: 0.4822\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2180 - val_loss: 0.4894\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2124 - val_loss: 0.4874\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2065 - val_loss: 0.4896\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2005 - val_loss: 0.5000\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1952 - val_loss: 0.4977\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1897 - val_loss: 0.5042\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1850 - val_loss: 0.5068\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1798 - val_loss: 0.5124\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1754 - val_loss: 0.5183\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1709 - val_loss: 0.5178\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1665 - val_loss: 0.5248\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1625 - val_loss: 0.5295\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1584 - val_loss: 0.5300\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1543 - val_loss: 0.5396\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1510 - val_loss: 0.5429\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1472 - val_loss: 0.5493\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1438 - val_loss: 0.5518\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1407 - val_loss: 0.5590\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1374 - val_loss: 0.5664\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1340 - val_loss: 0.5728\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1315 - val_loss: 0.5735\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1286 - val_loss: 0.5819\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1259 - val_loss: 0.5870\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1233 - val_loss: 0.5967\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1207 - val_loss: 0.5907\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1182 - val_loss: 0.5960\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1157 - val_loss: 0.6066\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1135 - val_loss: 0.6108\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1114 - val_loss: 0.6196\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1093 - val_loss: 0.6195\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1068 - val_loss: 0.6236\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1048 - val_loss: 0.6307\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1030 - val_loss: 0.6325\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1012 - val_loss: 0.6395\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0994 - val_loss: 0.6445\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0973 - val_loss: 0.6506\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0955 - val_loss: 0.6542\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0940 - val_loss: 0.6548\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0921 - val_loss: 0.6617\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0909 - val_loss: 0.6662\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0892 - val_loss: 0.6715\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0875 - val_loss: 0.6765\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0861 - val_loss: 0.6834\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0847 - val_loss: 0.6864\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.0837 - val_loss: 0.6905\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0820 - val_loss: 0.6970\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0805 - val_loss: 0.6920\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0795 - val_loss: 0.6994\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0780 - val_loss: 0.7107\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.0769 - val_loss: 0.7165\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0758 - val_loss: 0.7171\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0745 - val_loss: 0.7190\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0736 - val_loss: 0.7177\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.0722 - val_loss: 0.7203\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0715 - val_loss: 0.7279\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0700 - val_loss: 0.7368\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0690 - val_loss: 0.7309\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0683 - val_loss: 0.7419\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0669 - val_loss: 0.7392\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0664 - val_loss: 0.7450\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0652 - val_loss: 0.7499\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0642 - val_loss: 0.7529\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0635 - val_loss: 0.7584\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0626 - val_loss: 0.7590\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0618 - val_loss: 0.7542\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.0609 - val_loss: 0.7635\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0600 - val_loss: 0.7701\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0589 - val_loss: 0.7637\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0584 - val_loss: 0.7799\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0579 - val_loss: 0.7799\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0570 - val_loss: 0.7720\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.0563 - val_loss: 0.7883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nls3/anaconda2/envs/keras/lib/python3.6/site-packages/keras/engine/topology.py:2344: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# 定义整个模型\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 定义回调函数\n",
    "#callback_list = [callbacks.EarlyStopping(patience=10)]\n",
    "# 编译模型\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# 训练\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "# 保存模型\n",
    "model.save('s2s_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立推断模型\n",
    "\n",
    "+ 将推断模型与训练模型分开，这样比较清楚，但是他们两内部所用到的结构是一样的\n",
    "+ 推断的步骤如下\n",
    "    1. 将输入编码，得到解码器所需要的初始状态\n",
    "    2. 结合初始状态，对一个size=1的序列(其中只包含开始字符)做模型推断，得到的输出作为下一个size=1序列的内容\n",
    "    3. 结合当前的输出以及状态，重复以上步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 sampling 模型\n",
    "# 定义 encoder 模型，得到输出encoder_states\n",
    "encoder_model = Model(encoder_inputs, encoder_state)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 得到解码器的输出以及中间状态\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs]+decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 数字->字符 的字典，用于恢复\n",
    "reverse_input_char_index = dict([(i, char) for char, i in input_token_index.items()])\n",
    "reverse_target_char_index = dict([(i, char) for char, i in target_token_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 将输入序列进行编码\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # 生成一个size=1的空序列\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # 将这个空序列的内容设置为开始字符\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    \n",
    "    # 进行字符恢复\n",
    "    # 简单起见，假设batch_size = 1\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        # 退出条件：生成 \\n 或者 超过最大序列长度\n",
    "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length :\n",
    "            stop_condition = True\n",
    "            \n",
    "        # 更新target_seq\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        # 更新中间状态\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Come alone.\n",
      "Decoded sentence: Venez seuls !\n",
      "\n",
      "-\n",
      "Input sentence: Come alone.\n",
      "Decoded sentence: Venez seuls !\n",
      "\n",
      "-\n",
      "Input sentence: Come along.\n",
      "Decoded sentence: Venez seul !\n",
      "\n",
      "-\n",
      "Input sentence: Come on in!\n",
      "Decoded sentence: Arrête de te li peis travant.\n",
      "\n",
      "-\n",
      "Input sentence: Come on in.\n",
      "Decoded sentence: Entre.\n",
      "\n",
      "-\n",
      "Input sentence: Come quick!\n",
      "Decoded sentence: Dépêche-toi de venir !\n",
      "\n",
      "-\n",
      "Input sentence: Come quick!\n",
      "Decoded sentence: Dépêche-toi de venir !\n",
      "\n",
      "-\n",
      "Input sentence: Come to me.\n",
      "Decoded sentence: Venez à moi.\n",
      "\n",
      "-\n",
      "Input sentence: Come to us.\n",
      "Decoded sentence: Venez à nous.\n",
      "\n",
      "-\n",
      "Input sentence: Cut it out!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Cut it out!\n",
      "Decoded sentence: Arrête !\n",
      "\n",
      "-\n",
      "Input sentence: Cut it out.\n",
      "Decoded sentence: Assez !\n",
      "\n",
      "-\n",
      "Input sentence: Did we win?\n",
      "Decoded sentence: Avons-nous gagné ?\n",
      "\n",
      "-\n",
      "Input sentence: Do I stink?\n",
      "Decoded sentence: Est-ce que je pue ?\n",
      "\n",
      "-\n",
      "Input sentence: Do come in!\n",
      "Decoded sentence: Je vous en prie, en re peux !\n",
      "\n",
      "-\n",
      "Input sentence: Do men cry?\n",
      "Decoded sentence: Les hommes pleurent-ils ?\n",
      "\n",
      "-\n",
      "Input sentence: Don't fret.\n",
      "Decoded sentence: Ne te tracasse pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't fret.\n",
      "Decoded sentence: Ne te tracasse pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't fret.\n",
      "Decoded sentence: Ne te tracasse pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't fret.\n",
      "Decoded sentence: Ne te tracasse pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't jump!\n",
      "Decoded sentence: Ne saute pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't jump!\n",
      "Decoded sentence: Ne saute pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't move!\n",
      "Decoded sentence: Ne bouge pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't move!\n",
      "Decoded sentence: Ne bouge pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't move.\n",
      "Decoded sentence: Ne bougez pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't move.\n",
      "Decoded sentence: Ne bougez pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't rush.\n",
      "Decoded sentence: Ne te precis pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't rush.\n",
      "Decoded sentence: Ne te precis pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't sing.\n",
      "Decoded sentence: Ne chante pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't sing.\n",
      "Decoded sentence: Ne chante pas.\n",
      "\n",
      "-\n",
      "Input sentence: Don't talk!\n",
      "Decoded sentence: Ne parle pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't talk!\n",
      "Decoded sentence: Ne parle pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't talk.\n",
      "Decoded sentence: Ne parle pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't talk.\n",
      "Decoded sentence: Ne parle pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't wait.\n",
      "Decoded sentence: N'attends pas !\n",
      "\n",
      "-\n",
      "Input sentence: Don't wait.\n",
      "Decoded sentence: N'attends pas !\n",
      "\n",
      "-\n",
      "Input sentence: Duty calls.\n",
      "Decoded sentence: Le devoir m'appelle.\n",
      "\n",
      "-\n",
      "Input sentence: Fill it up.\n",
      "Decoded sentence: Le plein.\n",
      "\n",
      "-\n",
      "Input sentence: Find a job.\n",
      "Decoded sentence: Trouve un emploi !\n",
      "\n",
      "-\n",
      "Input sentence: Find a job.\n",
      "Decoded sentence: Trouve un emploi !\n",
      "\n",
      "-\n",
      "Input sentence: Follow Tom.\n",
      "Decoded sentence: Suis Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Follow Tom.\n",
      "Decoded sentence: Suis Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Follow him.\n",
      "Decoded sentence: Suis-le !\n",
      "\n",
      "-\n",
      "Input sentence: Follow him.\n",
      "Decoded sentence: Suis-le !\n",
      "\n",
      "-\n",
      "Input sentence: Forget Tom.\n",
      "Decoded sentence: Oubliez Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Forget Tom.\n",
      "Decoded sentence: Oubliez Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Forget him.\n",
      "Decoded sentence: Oublie-le.\n",
      "\n",
      "-\n",
      "Input sentence: Forget him.\n",
      "Decoded sentence: Oublie-le.\n",
      "\n",
      "-\n",
      "Input sentence: Forgive me.\n",
      "Decoded sentence: Pardonnez-moi.\n",
      "\n",
      "-\n",
      "Input sentence: Forgive us.\n",
      "Decoded sentence: Pardonne-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Get a life!\n",
      "Decoded sentence: Achète-toi une vie !\n",
      "\n",
      "-\n",
      "Input sentence: Get to bed.\n",
      "Decoded sentence: Allez au lit !\n",
      "\n",
      "-\n",
      "Input sentence: Get to bed.\n",
      "Decoded sentence: Allez au lit !\n",
      "\n",
      "-\n",
      "Input sentence: Get to bed.\n",
      "Decoded sentence: Allez au lit !\n",
      "\n",
      "-\n",
      "Input sentence: Give it up.\n",
      "Decoded sentence: Abandonne !\n",
      "\n",
      "-\n",
      "Input sentence: Give it up.\n",
      "Decoded sentence: Abandonne !\n",
      "\n",
      "-\n",
      "Input sentence: Go warm up.\n",
      "Decoded sentence: Va t'échauffer !\n",
      "\n",
      "-\n",
      "Input sentence: Go warm up.\n",
      "Decoded sentence: Va t'échauffer !\n",
      "\n",
      "-\n",
      "Input sentence: He gave in.\n",
      "Decoded sentence: Il céda.\n",
      "\n",
      "-\n",
      "Input sentence: He gave in.\n",
      "Decoded sentence: Il céda.\n",
      "\n",
      "-\n",
      "Input sentence: He hung up.\n",
      "Decoded sentence: Il a raccroché.\n",
      "\n",
      "-\n",
      "Input sentence: He hung up.\n",
      "Decoded sentence: Il a raccroché.\n",
      "\n",
      "-\n",
      "Input sentence: He is busy.\n",
      "Decoded sentence: Il est fauillé.\n",
      "\n",
      "-\n",
      "Input sentence: He is here!\n",
      "Decoded sentence: Il est ici !\n",
      "\n",
      "-\n",
      "Input sentence: He is kind.\n",
      "Decoded sentence: Il est heureux.\n",
      "\n",
      "-\n",
      "Input sentence: He is late.\n",
      "Decoded sentence: Il est paresseux.\n",
      "\n",
      "-\n",
      "Input sentence: He is lazy.\n",
      "Decoded sentence: Il est paresseux.\n",
      "\n",
      "-\n",
      "Input sentence: He is lazy.\n",
      "Decoded sentence: Il est paresseux.\n",
      "\n",
      "-\n",
      "Input sentence: He is poor.\n",
      "Decoded sentence: Il est pauvre.\n",
      "\n",
      "-\n",
      "Input sentence: He is sick.\n",
      "Decoded sentence: Il est jauneux.\n",
      "\n",
      "-\n",
      "Input sentence: He made it.\n",
      "Decoded sentence: Il a réussi.\n",
      "\n",
      "-\n",
      "Input sentence: He's Swiss.\n",
      "Decoded sentence: Il est tellement idiot !\n",
      "\n",
      "-\n",
      "Input sentence: He's Swiss.\n",
      "Decoded sentence: Il est tellement idiot !\n",
      "\n",
      "-\n",
      "Input sentence: He's broke.\n",
      "Decoded sentence: Il est trop saoul.\n",
      "\n",
      "-\n",
      "Input sentence: He's broke.\n",
      "Decoded sentence: Il est trop saoul.\n",
      "\n",
      "-\n",
      "Input sentence: He's drunk.\n",
      "Decoded sentence: Il est ivre.\n",
      "\n",
      "-\n",
      "Input sentence: He's drunk.\n",
      "Decoded sentence: Il est ivre.\n",
      "\n",
      "-\n",
      "Input sentence: He's smart.\n",
      "Decoded sentence: Il est intelligent.\n",
      "\n",
      "-\n",
      "Input sentence: Here he is!\n",
      "Decoded sentence: Il est ici !\n",
      "\n",
      "-\n",
      "Input sentence: Here it is.\n",
      "Decoded sentence: Le voici.\n",
      "\n",
      "-\n",
      "Input sentence: Here it is.\n",
      "Decoded sentence: Le voici.\n",
      "\n",
      "-\n",
      "Input sentence: Here it is.\n",
      "Decoded sentence: Le voici.\n",
      "\n",
      "-\n",
      "Input sentence: Here we go.\n",
      "Decoded sentence: On est partis !\n",
      "\n",
      "-\n",
      "Input sentence: Here we go.\n",
      "Decoded sentence: On est partis !\n",
      "\n",
      "-\n",
      "Input sentence: Hold still.\n",
      "Decoded sentence: Tenez-vous tranquille !\n",
      "\n",
      "-\n",
      "Input sentence: Hold still.\n",
      "Decoded sentence: Tenez-vous tranquille !\n",
      "\n",
      "-\n",
      "Input sentence: How lovely!\n",
      "Decoded sentence: Comme c'est charmant !\n",
      "\n",
      "-\n",
      "Input sentence: How's work?\n",
      "Decoded sentence: Comment va le tent ?\n",
      "\n",
      "-\n",
      "Input sentence: Hurry home.\n",
      "Decoded sentence: Dépêche-toi d'aller chez toi !\n",
      "\n",
      "-\n",
      "Input sentence: Hurry home.\n",
      "Decoded sentence: Dépêche-toi d'aller chez toi !\n",
      "\n",
      "-\n",
      "Input sentence: I am a man.\n",
      "Decoded sentence: Je suis égale.\n",
      "\n",
      "-\n",
      "Input sentence: I am human.\n",
      "Decoded sentence: J'ai faim !\n",
      "\n",
      "-\n",
      "Input sentence: I am ready.\n",
      "Decoded sentence: Je suis paresseuse.\n",
      "\n",
      "-\n",
      "Input sentence: I am right.\n",
      "Decoded sentence: Je suis chauve.\n",
      "\n",
      "-\n",
      "Input sentence: I am sorry.\n",
      "Decoded sentence: Je suis sûr.\n",
      "\n",
      "-\n",
      "Input sentence: I am sorry.\n",
      "Decoded sentence: Je suis sûr.\n",
      "\n",
      "-\n",
      "Input sentence: I am tired.\n",
      "Decoded sentence: Je suis chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: I am tired.\n",
      "Decoded sentence: Je suis chez lui.\n",
      "\n",
      "-\n",
      "Input sentence: I broke it.\n",
      "Decoded sentence: Je l'ai cassé.\n",
      "\n",
      "-\n",
      "Input sentence: I broke it.\n",
      "Decoded sentence: Je l'ai cassé.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 检验成果的时候到了,从训练集中选取一些句子做测试\n",
    "# 效果还行（废话，从训练集里挑的数据）\n",
    "for seq_index in range(1000, 1100):\n",
    "    # batch_size = 1\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    \n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "我们了解了如何用Keras实现Seq2Seq模型，关键在于构建编码器和解码器，并且要认识到训练阶段与推断阶段工作的流程是不一样的\n",
    "\n",
    "原文最后还提到了如何将我们这个字符级别的模型改成单词级别，有兴趣的同学可以了解了解"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
