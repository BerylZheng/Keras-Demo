{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "#set_session(tf.Session(config=config))\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import callbacks\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.wrappers import scikit_learn\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiying movie reviews: IMDB, a binary classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这个例子整理自[《Deep Learning with Python》](https://book.douban.com/subject/27038207/) 3.4节"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.探索数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:(25000,), training labels shape:(25000,)\n",
      "Test data shaep:(25000,), test labels shape:(25000,)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "\n",
    "# 只选择出现频率最多的前 10000 个单词\n",
    "max_features = 10000\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print('Training data shape:{}, training labels shape:{}'.format(x_train.shape, y_train.shape))\n",
    "print('Test data shaep:{}, test labels shape:{}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.   Length    Content(first 10 words)                                     Targets   \n",
      "0     195       [1, 103, 319, 14, 22, 13, 8033, 8, 61, 719]                 Negative  \n",
      "1     211       [1, 2894, 2, 2, 9, 6, 307, 21, 5129, 22]                    Positive  \n",
      "2     104       [1, 14, 9, 6, 55, 163, 20, 13, 28, 57]                      Positive  \n",
      "3     120       [1, 4, 1132, 5, 2914, 26, 574, 11, 4, 644]                  Positive  \n",
      "4     330       [1, 447, 4, 204, 65, 69, 55, 312, 1398, 18]                 Negative  \n",
      "5     319       [1, 14, 20, 739, 8, 28, 77, 35, 23, 4]                      Negative  \n",
      "6     253       [1, 146, 35, 2, 2, 5, 2, 5, 16, 1346]                       Negative  \n",
      "7     80        [1, 737, 20, 261, 13, 104, 12, 69, 4, 986]                  Positive  \n",
      "8     820       [1, 1065, 3184, 523, 2, 31, 7, 4, 91, 1149]                 Negative  \n",
      "9     98        [1, 4, 20, 165, 47, 6, 1018, 52, 65, 21]                    Positive  \n",
      "10    112       [1, 13, 66, 40, 14, 22, 54, 13, 645, 8]                     Positive  \n",
      "11    51        [1, 51, 517, 46, 17, 35, 221, 65, 946, 2]                   Negative  \n",
      "12    189       [1, 13, 16, 149, 14, 54, 61, 322, 446, 8]                   Positive  \n",
      "13    135       [1, 13, 546, 244, 6, 194, 337, 7, 364, 352]                 Negative  \n",
      "14    227       [1, 4, 2, 381, 5092, 3977, 17, 6, 3865, 46]                 Positive  \n",
      "15    122       [1, 14, 9, 24, 4, 801, 3774, 3228, 22, 12]                  Positive  \n",
      "16    189       [1, 13, 774, 1498, 14, 254, 33, 6, 20, 11]                  Positive  \n",
      "17    161       [1, 14, 20, 9, 35, 2152, 437, 7, 58, 4]                     Negative  \n",
      "18    418       [1, 785, 167, 1560, 2, 218, 618, 573, 18, 87]               Negative  \n",
      "19    126       [1, 48, 25, 70, 264, 12, 160, 604, 7, 2521]                 Negative  \n",
      "20    102       [1, 14, 20, 4342, 53, 4346, 3680, 7, 5536, 34]              Negative  \n",
      "21    700       [1, 13, 244, 179, 6, 337, 7, 7858, 2979, 488]               Negative  \n",
      "22    168       [1, 3281, 9, 1016, 2238, 625, 196, 416, 2, 336]             Negative  \n",
      "23    113       [1, 13, 6037, 1133, 14, 20, 17, 160, 338, 781]              Negative  \n",
      "24    139       [1, 14, 22, 9, 44, 6, 223, 269, 8, 216]                     Negative  \n",
      "25    126       [1, 17, 13, 16, 149, 14, 20, 13, 16, 536]                   Negative  \n",
      "26    154       [1, 857, 18, 14, 22, 71, 2, 33, 118, 137]                   Negative  \n",
      "27    205       [1, 73, 13, 215, 135, 15, 14, 16, 31, 609]                  Positive  \n",
      "28    180       [1, 14, 402, 8748, 2, 2196, 1477, 6, 1081, 5794]            Negative  \n",
      "29    103       [1, 4, 2050, 506, 16, 1812, 9988, 86, 22, 237]              Positive  \n",
      "30    256       [1, 6092, 5, 4156, 323, 17, 241, 3010, 18, 4]               Positive  \n",
      "31    121       [1, 35, 204, 2, 2492, 7, 14, 480, 22, 16]                   Positive  \n",
      "32    159       [1, 4, 64, 147, 2468, 11, 4, 20, 9, 4]                      Negative  \n",
      "33    83        [1, 14, 20, 144, 24, 2, 17, 438, 261, 12]                   Negative  \n",
      "34    72        [1, 4, 7568, 1530, 803, 50, 8, 135, 44, 12]                 Negative  \n",
      "35    303       [1, 138, 81, 13, 1343, 81, 14, 8, 546, 13]                  Negative  \n",
      "36    177       [1, 14, 592, 1648, 500, 9, 4, 118, 1648, 500]               Positive  \n",
      "37    131       [1, 61, 223, 5, 13, 7930, 9597, 4, 314, 159]                Negative  \n",
      "38    477       [1, 2, 8902, 16, 641, 153, 1404, 7, 27, 2]                  Positive  \n",
      "39    100       [1, 242, 162, 2, 249, 20, 126, 93, 10, 10]                  Negative  \n",
      "40    307       [1, 13, 219, 14, 20, 11, 6747, 1242, 187, 982]              Positive  \n",
      "41    227       [1, 14, 3720, 852, 2, 1545, 4, 6610, 5, 12]                 Positive  \n",
      "42    148       [1, 14, 9, 4, 91, 9453, 664, 13, 28, 126]                   Positive  \n",
      "43    297       [1, 14, 9, 51, 13, 1040, 8, 49, 369, 908]                   Positive  \n",
      "44    138       [1, 57, 1028, 133, 21, 13, 28, 77, 6, 337]                  Negative  \n",
      "45    194       [1, 13, 296, 14, 22, 1033, 23, 288, 5, 13]                  Negative  \n",
      "46    174       [1, 75, 28, 312, 1398, 19, 14, 31, 88, 94]                  Positive  \n",
      "47    101       [1, 1334, 418, 7, 52, 438, 14, 9, 31, 7]                    Positive  \n",
      "48    150       [1, 14, 9, 869, 31, 7, 4, 249, 102, 13]                     Negative  \n",
      "49    263       [1, 5259, 5, 2289, 28, 77, 6, 2, 7, 438]                    Negative  \n",
      "50    176       [1, 2, 9, 165, 6, 1491, 56, 11, 1303, 7]                    Negative  \n",
      "51    155       [1, 13, 104, 12, 16, 35, 3879, 3245, 2001, 595]             Negative  \n",
      "52    309       [1, 13, 2, 14, 5, 2, 457, 17, 4, 118]                       Positive  \n",
      "53    499       [1, 397, 8, 157, 23, 14, 22, 54, 12, 16]                    Positive  \n",
      "54    331       [1, 13, 16, 2229, 8, 842, 15, 294, 69, 93]                  Negative  \n",
      "55    45        [1, 13, 421, 1309, 83, 4, 182, 7, 4, 7673]                  Positive  \n",
      "56    220       [1, 160, 1295, 119, 5929, 22, 39, 2912, 24, 6]              Positive  \n",
      "57    138       [1, 608, 86, 7, 32, 4, 374, 272, 40, 12]                    Negative  \n",
      "58    165       [1, 14, 20, 9, 373, 33, 4, 130, 7, 12]                      Negative  \n",
      "59    313       [1, 75, 32, 28, 110, 49, 2, 7445, 11, 263]                  Negative  \n",
      "60    472       [1, 2891, 185, 7362, 1102, 39, 1831, 8, 162, 782]           Positive  \n",
      "61    121       [1, 14, 9, 35, 589, 34, 199, 2133, 4500, 7418]              Negative  \n",
      "62    499       [1, 33, 31, 130, 7, 4, 4277, 3864, 4517, 1075]              Positive  \n",
      "63    304       [1, 14, 3679, 2, 2, 11, 14, 20, 5, 11]                      Positive  \n",
      "64    99        [1, 13, 633, 40, 865, 102, 21, 14, 31, 16]                  Negative  \n",
      "65    161       [1, 4, 20, 2013, 56, 19, 6, 196, 686, 324]                  Negative  \n",
      "66    127       [1, 14, 22, 127, 6, 897, 292, 7, 5007, 4]                   Positive  \n",
      "67    332       [1, 198, 24, 43, 61, 1192, 6843, 23, 14, 22]                Negative  \n",
      "68    152       [1, 141, 6, 1917, 20, 55, 483, 4051, 31, 191]               Positive  \n",
      "69    112       [1, 321, 993, 708, 256, 76, 2, 5, 7569, 74]                 Positive  \n",
      "70    95        [1, 51, 9, 14, 9, 12, 6, 212, 6, 189]                       Negative  \n",
      "71    167       [1, 13, 43, 296, 14, 20, 23, 2, 387, 72]                    Negative  \n",
      "72    258       [1, 261, 4, 973, 18, 14, 22, 16, 5481, 572]                 Negative  \n",
      "73    96        [1, 14, 20, 16, 40, 4, 910, 1308, 103, 465]                 Negative  \n",
      "74    123       [1, 11, 35, 589, 8, 721, 145, 4, 1629, 1179]                Negative  \n",
      "75    140       [1, 2, 2, 6592, 9, 4, 2, 1094, 1162, 664]                   Positive  \n",
      "76    119       [1, 1065, 5121, 470, 4, 2, 9, 6, 93, 18]                    Negative  \n",
      "77    406       [1, 45, 254, 8, 376, 48, 2, 5, 5343, 26]                    Negative  \n",
      "78    261       [1, 10, 10, 261, 4, 485, 524, 9, 9546, 307]                 Negative  \n",
      "79    206       [1, 13, 286, 1097, 252, 51, 8, 535, 39, 6]                  Negative  \n",
      "80    328       [1, 13, 2, 69, 14, 20, 23, 61, 4200, 18]                    Negative  \n",
      "81    257       [1, 57, 9697, 466, 1115, 206, 10, 10, 1067, 1219]           Negative  \n",
      "82    460       [1, 86, 13, 144, 760, 15, 13, 66, 510, 2]                   Negative  \n",
      "83    178       [1, 103, 1790, 56, 11, 4, 2542, 1986, 7, 7190]              Positive  \n",
      "84    121       [1, 61, 2, 13, 258, 14, 20, 8, 30, 55]                      Negative  \n",
      "85    129       [1, 13, 377, 319, 14, 31, 54, 13, 16, 1542]                 Positive  \n",
      "86    191       [1, 2, 46, 7, 1827, 6, 1366, 7, 6751, 1298]                 Negative  \n",
      "87    586       [1, 13, 92, 104, 15, 111, 108, 262, 1290, 28]               Positive  \n",
      "88    227       [1, 103, 4, 1023, 7, 4, 333, 2, 745, 3378]                  Positive  \n",
      "89    126       [1, 1450, 7824, 830, 758, 2, 9, 6, 20, 15]                  Positive  \n",
      "90    239       [1, 31, 7, 4, 1126, 1936, 7, 2, 2115, 7]                    Positive  \n",
      "91    148       [1, 13, 473, 8, 40, 14, 22, 422, 94, 6]                     Negative  \n",
      "92    159       [1, 14, 20, 16, 373, 13, 69, 6, 55, 878]                    Negative  \n",
      "93    110       [1, 13, 219, 14, 145, 11, 2, 54, 12, 16]                    Negative  \n",
      "94    47        [1, 66, 45, 164, 76, 13, 64, 386, 149, 12]                  Negative  \n",
      "95    181       [1, 9, 12, 6, 52, 326, 8, 361, 412, 1389]                   Positive  \n",
      "96    377       [1, 4, 20, 778, 6, 19, 2985, 5235, 250, 19]                 Positive  \n",
      "97    55        [1, 321, 2542, 5, 283, 1146, 7, 2, 7033, 113]               Positive  \n",
      "98    54        [1, 13, 92, 124, 138, 21, 54, 13, 244, 1803]                Negative  \n",
      "99    81        [1, 316, 299, 68, 173, 184, 73, 11, 14, 117]                Positive  \n"
     ]
    }
   ],
   "source": [
    "# 显示部分数据\n",
    "#print('No.\\t\\tLength\\t\\tContent(the first 10 words)\\t\\t\\t\\tTarget')\n",
    "print('{:<6}{:<10}{:<60}{:<10}'.format('No.', 'Length', 'Content(first 10 words)', 'Targets'))\n",
    "for i, (x,y) in enumerate(zip(x_train[:100], y_train[:100])):\n",
    "    target = 'Positive'if y==1 else 'Negative'\n",
    "    print('{:<6}{:<10}{:<60}{:<10}'.format(i, len(x), str(x[:10]), target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels\n",
      "positive:12500, negative:12500\n",
      "Test labels\n",
      "positive:12500, negative:12500\n"
     ]
    }
   ],
   "source": [
    "# 统计正负数据的个数、比例\n",
    "\n",
    "# 训练数据\n",
    "train_labels_count = Counter(y_train)\n",
    "test_labels_count = Counter(y_test)\n",
    "print('Training labels\\npositive:{}, negative:{}'.format(train_labels_count[1], train_labels_count[0]))\n",
    "print('Test labels\\npositive:{}, negative:{}'.format(test_labels_count[1], test_labels_count[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max word index: 9999\n",
      "Min word index: 1\n",
      "? i rated this movie as awful 1 after watching the trailer i thought this movie could be pretty cool guaranteed to offend everyone the trailer said well it did offend me because this movie really sucks it is hardly a comedy as i laughed about two seconds during the entire movie and what's with all the gays in this movie i'm not gay and i don't have a problem with those who are but what's the point of adding so many gay scenes in a so called comedy movie when these scenes are absolutely not funny i guess the director is a gay man in denial or something like that br br so my advice to you is if you want to waste good money go rent a good comedy you've already seen a million times you'll be better off than watching this mother of all lousy ? it really is total crap\n"
     ]
    }
   ],
   "source": [
    "# 将数字转换为单词\n",
    "word_index = imdb.get_word_index()\n",
    "num_to_word = { value:key for (key, value) in word_index.items()}\n",
    "print('Max word index:', max([max(sequence) for sequence in x_train]))\n",
    "print('Min word index:', min([min(sequence) for sequence in x_train]))\n",
    "\n",
    "def decode_review(num_to_word, review):\n",
    "    # i - 3是因为0, 1, 2代表着'padding','start', 'unknow'， 因此单词的下标真正是从3开始的\n",
    "    decoded = ' '.join( [num_to_word.get(i-3, '?') for i in review])\n",
    "    return decoded\n",
    "\n",
    "print(decode_review(num_to_word, x_train[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.1 \t\t the\n",
      "No.2 \t\t and\n",
      "No.3 \t\t a\n",
      "No.4 \t\t of\n",
      "No.5 \t\t to\n",
      "No.6 \t\t is\n",
      "No.7 \t\t br\n",
      "No.8 \t\t in\n",
      "No.9 \t\t it\n",
      "No.10 \t\t i\n",
      "No.11 \t\t this\n",
      "No.12 \t\t that\n",
      "No.13 \t\t was\n",
      "No.14 \t\t as\n",
      "No.15 \t\t for\n",
      "No.16 \t\t with\n",
      "No.17 \t\t movie\n",
      "No.18 \t\t but\n",
      "No.19 \t\t film\n"
     ]
    }
   ],
   "source": [
    "# 打印出使用频率最高的一些单词\n",
    "for i in range(1, 20):\n",
    "    print('No.%d \\t\\t %s'%(i, num_to_word[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.结论\n",
    "+ 训练数据有25000条，训练数据的长度不是固定。测试数据25000条，测试数据长度不固定。因此需要将每条数据进行固定长度，太长的数据进行截断，太短的数据进行填充\n",
    "+ 单词转为数字才能作为网络的输入，数字越小说明单词出现频率越高，但是一些高频率单词，例如the, and , a之类的并没有提供有效的信息，因此可以考虑将这部分单词过滤，数字的范围 [1, max_feature]。 \n",
    "+ 样本的正负比例是1:1，无论是训练样本还是测试样本，都是12500条正样本， 12500条负样本\n",
    "+ 样本的排列已经是随机状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 定义问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 输入数据：$x\\in R^n$, $n$表示长度，$x_i$是一个数字，表示一个单词，不同的$x$，n的值不同\n",
    "+ 输出：$y\\in\\{0, 1\\}$, 0表示负面评价，1表示正面评价\n",
    "+ 问题归类：属于二元分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 衡量指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这是一个正负比例平衡的问题，因此选择Accuracy作为衡量模型指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 验证策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 数据够多，选择hold-out验证策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 过滤出现频率过高的单词，过滤前10%\n",
    "+ 固定样本长度，过长截断，过短填充\n",
    "+ 将数据向量化，有两种策略（两种都试试吧）：\n",
    "    1. 将$x$转换为一个$X\\in R^{M}$，M是单词最多个数，$X_i\\in\\{0, 1\\}$，1表示数字为$i$的单词出现在$x$中\n",
    "    2. 加入EMbedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单词个数\n",
    "max_features = 10000\n",
    "skips = 50\n",
    "\n",
    "# 导入数据，设置过滤个数，单词总数\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words=max_features, skip_top=skips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:(25000, 10000), x_test shaep:(25000, 10000)\n"
     ]
    }
   ],
   "source": [
    "# 向量化（策略1）\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "        \n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(x_train, max_features)\n",
    "x_test = vectorize_sequences(x_test, max_features)\n",
    "print('x_train shape:{}, x_test shaep:{}'.format(x_train.shape, x_test.shape) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标向量化\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 简单模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 该问题的base line 是0.5。建立一个简单的模型，准确率高于0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/20\n",
      "17500/17500 [==============================] - 9s 530us/step - loss: 0.4716 - acc: 0.7906 - val_loss: 0.3832 - val_acc: 0.8315\n",
      "Epoch 2/20\n",
      "17500/17500 [==============================] - 9s 507us/step - loss: 0.2629 - acc: 0.8949 - val_loss: 0.4079 - val_acc: 0.8223\n",
      "Epoch 3/20\n",
      "17500/17500 [==============================] - 9s 519us/step - loss: 0.1842 - acc: 0.9311 - val_loss: 0.4648 - val_acc: 0.8185\n",
      "Epoch 4/20\n",
      "17500/17500 [==============================] - 9s 521us/step - loss: 0.1307 - acc: 0.9549 - val_loss: 0.5308 - val_acc: 0.8133\n",
      "Epoch 5/20\n",
      "17500/17500 [==============================] - 9s 533us/step - loss: 0.0938 - acc: 0.9678 - val_loss: 0.6205 - val_acc: 0.8093\n",
      "Epoch 6/20\n",
      "17500/17500 [==============================] - 9s 526us/step - loss: 0.0662 - acc: 0.9803 - val_loss: 0.7154 - val_acc: 0.8067\n",
      "Epoch 7/20\n",
      "17500/17500 [==============================] - 9s 514us/step - loss: 0.0492 - acc: 0.9860 - val_loss: 0.8079 - val_acc: 0.8025\n",
      "Epoch 8/20\n",
      "17500/17500 [==============================] - 9s 513us/step - loss: 0.0348 - acc: 0.9914 - val_loss: 0.9046 - val_acc: 0.8000\n",
      "Epoch 9/20\n",
      "17500/17500 [==============================] - 9s 533us/step - loss: 0.0254 - acc: 0.9939 - val_loss: 0.9714 - val_acc: 0.8028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5cad8a9ef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_base_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Dense(16, activation='relu', input_shape=(max_features, )))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "# 建立模型\n",
    "base_model = build_base_model()\n",
    "# 回调函数\n",
    "callback_list = [callbacks.EarlyStopping(monitor='val_loss', patience=8),\n",
    "                callbacks.ModelCheckpoint('best_base_model.h5', save_best_only=True),\n",
    "                callbacks.TensorBoard('./logs', histogram_freq=1)]\n",
    "base_model.fit(x_train, y_train, epochs=20, batch_size=32, callbacks=callback_list, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 在验证集上准确率最高83%左右，但是一开始就过拟合了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 全面升级：开发一个过拟合的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 基本模型已经过拟合，这一步省略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 调整参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 尝试Dropout、Batch Normaliztion等\n",
    "+ 添加L1/L2正则化\n",
    "+ 尝试不同的网络结构，添加层或者删除层\n",
    "+ 尝试不同的超参数，例如神经元的个数，batch_size等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dropout=0.2, L=0.001):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Dense(16,\n",
    "                           activation='relu', kernel_initializer='he_normal', \n",
    "                           input_shape=(max_features, )))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    model.add(layers.BatchNormalization()) \n",
    "    model.add(layers.Dense(16,activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    model.add(layers.BatchNormalization()) \n",
    "    #model.add(layers.Dense(16, activation='relu', kernel_initializer='he_normal'))\n",
    "    #model.add(layers.Dropout(dropout))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid', kernel_initializer='he_normal'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 10s 478us/step - loss: 0.5407 - acc: 0.7213 - val_loss: 0.3559 - val_acc: 0.8792\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 9s 440us/step - loss: 0.3554 - acc: 0.8560 - val_loss: 0.2814 - val_acc: 0.8870\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 8s 408us/step - loss: 0.2865 - acc: 0.8887 - val_loss: 0.2762 - val_acc: 0.8900\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 8s 423us/step - loss: 0.2525 - acc: 0.9026 - val_loss: 0.2788 - val_acc: 0.8894\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 8s 398us/step - loss: 0.2193 - acc: 0.9168 - val_loss: 0.2879 - val_acc: 0.8910\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 8s 424us/step - loss: 0.2068 - acc: 0.9197 - val_loss: 0.2954 - val_acc: 0.8884\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 8s 404us/step - loss: 0.1847 - acc: 0.9280 - val_loss: 0.3156 - val_acc: 0.8872\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 8s 418us/step - loss: 0.1756 - acc: 0.9320 - val_loss: 0.3034 - val_acc: 0.8876\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 9s 444us/step - loss: 0.1626 - acc: 0.9365 - val_loss: 0.3270 - val_acc: 0.8862\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 10s 479us/step - loss: 0.1655 - acc: 0.9342 - val_loss: 0.3384 - val_acc: 0.8844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa095863c18>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.clear_session()\n",
    "dropout = 0.5\n",
    "L = 0.001\n",
    "model = create_model(dropout, L)\n",
    "\n",
    "# 回调函数\n",
    "callback_list = [callbacks.EarlyStopping(monitor='val_loss', patience=7),\n",
    "                callbacks.ModelCheckpoint('best_model.h5', save_best_only=True),\n",
    "                callbacks.TensorBoard('./logs', histogram_freq=1)]\n",
    "\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=64, callbacks=callback_list, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 6s 249us/step - loss: 0.5115 - acc: 0.7449\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 6s 234us/step - loss: 0.3438 - acc: 0.8586\n",
      "Epoch 3/3\n",
      "24448/25000 [============================>.] - ETA: 0s - loss: 0.2856 - acc: 0.8876"
     ]
    }
   ],
   "source": [
    "# 调整参数后，利用所有数据进行训练\n",
    "model = create_model(dropout, L)\n",
    "# 回调函数\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64)\n",
    "model.evaluate(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 133us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29331848189353943, 0.87672000013351437]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入最优模型\n",
    "model = models.load_model('best_model.h5')\n",
    "model.evaluate(x_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
